{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised anomaly detection with RNN autoencoders based on LSTM cells, with some statistical data filtering\n",
    "\n",
    "### Case: The Stolen Szechuan Sauce\n",
    "\n",
    "Since the logs are presented in a chronological order, it is reasonable to assume that an anomaly isn't just a single event, but a sequence of events. This is why we will use a recurrent neural network (RNN) to detect anomalies. The RNN will be an autoencoder, which means that it will learn to reconstruct the input sequence. The reconstruction error will be used to detect anomalies.\n",
    "\n",
    "#### The implimentation is based on tensorflow\n",
    "\n",
    "First, we load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lupas\\AppData\\Local\\Temp\\ipykernel_936\\2434507779.py:6: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('./data/dc_file_modified2.csv')\n",
      "C:\\Users\\lupas\\AppData\\Local\\Temp\\ipykernel_936\\2434507779.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_data[\"inode + filename\"] = sub_data['inode'].astype(str) +\" - \"+ sub_data[\"filename\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# loading the data\n",
    "data = pd.read_csv('./data/dc_file_modified2.csv')\n",
    "\n",
    "# filtering out the unnecesaary columns\n",
    "sub_data = data[[\n",
    "            'inode', \n",
    "            'M',\n",
    "            'A',\n",
    "            'C',\n",
    "            'B', \n",
    "            'file_stat',\n",
    "            'NTFS_file_stat',\n",
    "            'file_entry_shell_item',\n",
    "            'NTFS_USN_change', 'filef',\n",
    "            'directory',\n",
    "            'link', \n",
    "            'dir_appdata', \n",
    "            'dir_win', \n",
    "            'dir_user',\n",
    "            'dir_other',\n",
    "            'file_executable',\n",
    "            'file_graphic',\n",
    "            'file_documents',\n",
    "            'file_ps', \n",
    "            'file_other', \n",
    "            'mft', \n",
    "            'lnk_shell_items',\n",
    "            'olecf_olecf_automatic_destinations/lnk/shell_items',\n",
    "            'winreg_bagmru/shell_items',\n",
    "            'usnjrnl', \n",
    "            'is_allocated1',\n",
    "            'is_allocated0',\n",
    "            'filename'\n",
    "            ]]\n",
    "\n",
    "# reshaping the columns\n",
    "sub_data[\"inode + filename\"] = sub_data['inode'].astype(str) +\" - \"+ sub_data[\"filename\"]\n",
    "inodes = sub_data['inode'].astype(int).to_list()\n",
    "sub_data = sub_data.drop(['inode'], axis=1)\n",
    "sub_data = sub_data.drop(['filename'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the noise. Namely, we ignore all the entries from inodes < 100, which are mostly system files.\n",
    "Also, we get rid of the inode `84656`, which is responsible for journaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = sub_data['inode + filename'].to_list()\n",
    "sub_data = sub_data.drop(['inode + filename'], axis=1)\n",
    "sub_data = sub_data.to_numpy(dtype=np.float32)  # converting to NumPy\n",
    "\n",
    "boring_indodes = set(list(range(100)) + [84656])\n",
    "\n",
    "good_data = []\n",
    "good_file_names = []\n",
    "\n",
    "for i in range(len(sub_data)):\n",
    "    if inodes[i] not in boring_indodes:\n",
    "        good_data.append(sub_data[i])\n",
    "        good_file_names.append(file_names[i])\n",
    "\n",
    "sub_data = np.array(good_data)\n",
    "file_names = good_file_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to build the model\n",
    "\n",
    "We start by defyning a minimalistic autoencoder layer, which uses LSTM cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the data to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sub_data.reshape((sub_data.shape[0], 1, sub_data.shape[1]))\n",
    "\n",
    "# shifting the targets by 1, so that the model can predict the next value\n",
    "target_data = np.concatenate((np.expand_dims(sub_data[0], axis=0), sub_data[1:]), axis=0)\n",
    "target_data = target_data.reshape((target_data.shape[0], 1, target_data.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from online_autoencoder import OnlineLSTMAutoencoder, ReconstructionLoss\n",
    "\n",
    "# Creating and compiling the model\n",
    "inputs = tf.keras.Input(shape=(1, input_data.shape[-1]))\n",
    "outputs = OnlineLSTMAutoencoder(\n",
    "    timesteps=50, features=input_data.shape[-1], encoding_dim=248,\n",
    ")(inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=ReconstructionLoss(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model.\n",
    "\n",
    "Since we uses batches of size 1 in order to preserve the sequence order, we train the model for only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9947/9947 [==============================] - 272s 26ms/step - loss: 0.0752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7349a2690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=input_data, y=target_data, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9947/9947 [==============================] - 71s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "with open('./outputs/anomalies.csv', 'w') as file:\n",
    "    predictions = model.predict(input_data)\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        mse = loss(target_data[i], predictions[i])\n",
    "\n",
    "        try:\n",
    "            writer.writerow([mse.numpy(), file_names[i+1].strip()])\n",
    "        except:\n",
    "            pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
